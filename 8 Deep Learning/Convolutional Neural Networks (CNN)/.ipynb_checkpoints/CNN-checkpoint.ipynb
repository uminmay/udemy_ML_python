{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "35f1a4e4-2455-4b03-ab18-88664dc3998e",
   "metadata": {},
   "source": [
    "# Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e090feb-e8ad-4981-93bf-dd4fda384cad",
   "metadata": {},
   "source": [
    "#### Importing the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8627dd1a-e02b-45d7-803f-296244161304",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f779a564-80de-4dde-9199-4e60a1b0f1c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.3.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d97e902-e78e-489d-81f1-f334daaa2356",
   "metadata": {},
   "source": [
    "### 1 Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ba4da7e-692a-41c3-8d2a-806d9aa29977",
   "metadata": {},
   "source": [
    "#### Preprocessing the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dfe87133-85cc-45a3-9f1a-040d0ee22407",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_datagen = ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True)\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        'dataset/training_set',\n",
    "        target_size=(64, 64),\n",
    "        batch_size=32,\n",
    "        class_mode='binary')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "716db60f-1643-4df3-9429-17b744e64ada",
   "metadata": {},
   "source": [
    "#### Preprocessing the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5ab43334-d259-4805-9b58-7365b982f552",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "        'dataset/test_set',\n",
    "        target_size=(64, 64),\n",
    "        batch_size=32,\n",
    "        class_mode='binary')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33232170-7e12-477e-b104-72333bd0aa03",
   "metadata": {},
   "source": [
    "### 2 Building the CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1f629904-59e3-4841-aa96-5701c53c6162",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn = tf.keras.models.Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38a4295d-1d50-4342-88b4-92d75788f017",
   "metadata": {},
   "source": [
    "#### 2.1 Convolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c72ad156-08f8-47e0-b7fb-1302679376ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.Conv2D(filters = 32, kernel_size = 3, activation = 'relu', input_shape=[64, 64, 3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "791805f5-5b17-4f5e-8d09-84718899dec3",
   "metadata": {},
   "source": [
    "#### 2.2 Pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "711c9b47-289a-489e-be4d-bdb081e59b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.MaxPool2D(pool_size = 2, strides = 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5afb09ea-44bd-4e27-bd93-a4d46900d764",
   "metadata": {},
   "source": [
    "#### Adding a second convolution layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b5a42e77-2a70-45bd-83a8-610b0b8a861b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.Conv2D(filters = 32, kernel_size = 3, activation = 'relu'))\n",
    "cnn.add(tf.keras.layers.MaxPool2D(pool_size = 2, strides = 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7766720a-1ded-40ed-8b7e-fdae2313acc2",
   "metadata": {},
   "source": [
    "#### 2.3 Flattening"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7aaaaa67-b17a-4035-b06f-4ebf3a0e7dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.Flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "806df133-a98b-4db7-99ec-d977661547ba",
   "metadata": {},
   "source": [
    "#### 2.4 Full Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a3bfe0e1-21c3-439c-a0c6-b2e779849369",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.Dense(units = 128, activation = 'relu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cfd5dfb-cdad-4971-8afd-838dd96a1d44",
   "metadata": {},
   "source": [
    "#### 2.5 Output Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f78b419c-f74b-47da-9337-13447327331a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.Dense(units = 1, activation = 'sigmoid')) # softmax in multiclass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e313045-ea25-4c8a-a884-ff713200cdd0",
   "metadata": {},
   "source": [
    "### 3 Training the CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b7fefd8-8d04-4094-8bcc-52cbb5be8545",
   "metadata": {},
   "source": [
    "#### Compiling the CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "41a749e3-a7d8-48a2-8287-03e6b931d28d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics  = ['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b01eac7-b894-4fb0-89b7-92a46ff1e1d7",
   "metadata": {},
   "source": [
    "#### Training the CNN on Training set and evaluating on the Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a2cfed37-09ff-4dc8-a4ce-54ec7fba2109",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "250/250 [==============================] - 88s 352ms/step - loss: 0.6607 - accuracy: 0.5954 - val_loss: 0.6345 - val_accuracy: 0.6310\n",
      "Epoch 2/30\n",
      "250/250 [==============================] - 89s 355ms/step - loss: 0.6079 - accuracy: 0.6680 - val_loss: 0.6135 - val_accuracy: 0.6705\n",
      "Epoch 3/30\n",
      "250/250 [==============================] - 88s 353ms/step - loss: 0.5700 - accuracy: 0.7011 - val_loss: 0.5950 - val_accuracy: 0.6625\n",
      "Epoch 4/30\n",
      "250/250 [==============================] - 88s 351ms/step - loss: 0.5411 - accuracy: 0.7247 - val_loss: 0.5229 - val_accuracy: 0.7390\n",
      "Epoch 5/30\n",
      "250/250 [==============================] - 92s 369ms/step - loss: 0.5197 - accuracy: 0.7449 - val_loss: 0.5462 - val_accuracy: 0.7390\n",
      "Epoch 6/30\n",
      "250/250 [==============================] - 88s 353ms/step - loss: 0.4920 - accuracy: 0.7571 - val_loss: 0.5037 - val_accuracy: 0.7625\n",
      "Epoch 7/30\n",
      "250/250 [==============================] - 78s 311ms/step - loss: 0.4818 - accuracy: 0.7690 - val_loss: 0.4529 - val_accuracy: 0.7960\n",
      "Epoch 8/30\n",
      "250/250 [==============================] - 68s 272ms/step - loss: 0.4603 - accuracy: 0.7814 - val_loss: 0.4777 - val_accuracy: 0.7670\n",
      "Epoch 9/30\n",
      "250/250 [==============================] - 70s 279ms/step - loss: 0.4405 - accuracy: 0.7919 - val_loss: 0.4853 - val_accuracy: 0.7710\n",
      "Epoch 10/30\n",
      "250/250 [==============================] - 68s 273ms/step - loss: 0.4326 - accuracy: 0.7959 - val_loss: 0.4489 - val_accuracy: 0.7855\n",
      "Epoch 11/30\n",
      "250/250 [==============================] - 67s 269ms/step - loss: 0.4099 - accuracy: 0.8108 - val_loss: 0.4334 - val_accuracy: 0.8040\n",
      "Epoch 12/30\n",
      "250/250 [==============================] - 69s 277ms/step - loss: 0.3936 - accuracy: 0.8180 - val_loss: 0.4467 - val_accuracy: 0.7945\n",
      "Epoch 13/30\n",
      "250/250 [==============================] - 67s 269ms/step - loss: 0.3865 - accuracy: 0.8244 - val_loss: 0.4262 - val_accuracy: 0.8005\n",
      "Epoch 14/30\n",
      "250/250 [==============================] - 70s 279ms/step - loss: 0.3750 - accuracy: 0.8280 - val_loss: 0.4513 - val_accuracy: 0.7915\n",
      "Epoch 15/30\n",
      "250/250 [==============================] - 69s 275ms/step - loss: 0.3646 - accuracy: 0.8380 - val_loss: 0.4521 - val_accuracy: 0.7940\n",
      "Epoch 16/30\n",
      "250/250 [==============================] - 68s 273ms/step - loss: 0.3416 - accuracy: 0.8474 - val_loss: 0.4936 - val_accuracy: 0.8045\n",
      "Epoch 17/30\n",
      "250/250 [==============================] - 68s 271ms/step - loss: 0.3292 - accuracy: 0.8574 - val_loss: 0.4063 - val_accuracy: 0.8240\n",
      "Epoch 18/30\n",
      "250/250 [==============================] - 67s 268ms/step - loss: 0.3196 - accuracy: 0.8581 - val_loss: 0.4130 - val_accuracy: 0.8105\n",
      "Epoch 19/30\n",
      "250/250 [==============================] - 70s 279ms/step - loss: 0.3154 - accuracy: 0.8622 - val_loss: 0.4706 - val_accuracy: 0.8105\n",
      "Epoch 20/30\n",
      "250/250 [==============================] - 67s 268ms/step - loss: 0.2975 - accuracy: 0.8710 - val_loss: 0.4455 - val_accuracy: 0.8050\n",
      "Epoch 21/30\n",
      "250/250 [==============================] - 68s 272ms/step - loss: 0.2945 - accuracy: 0.8680 - val_loss: 0.4674 - val_accuracy: 0.8160\n",
      "Epoch 22/30\n",
      "250/250 [==============================] - 67s 270ms/step - loss: 0.2784 - accuracy: 0.8829 - val_loss: 0.4481 - val_accuracy: 0.8115\n",
      "Epoch 23/30\n",
      "250/250 [==============================] - 68s 271ms/step - loss: 0.2600 - accuracy: 0.8882 - val_loss: 0.4888 - val_accuracy: 0.8020\n",
      "Epoch 24/30\n",
      "250/250 [==============================] - 70s 279ms/step - loss: 0.2479 - accuracy: 0.8957 - val_loss: 0.4635 - val_accuracy: 0.8125\n",
      "Epoch 25/30\n",
      "250/250 [==============================] - 72s 288ms/step - loss: 0.2350 - accuracy: 0.9024 - val_loss: 0.4846 - val_accuracy: 0.8085\n",
      "Epoch 26/30\n",
      "250/250 [==============================] - 68s 271ms/step - loss: 0.2237 - accuracy: 0.9031 - val_loss: 0.4970 - val_accuracy: 0.8120\n",
      "Epoch 27/30\n",
      "250/250 [==============================] - 67s 267ms/step - loss: 0.2204 - accuracy: 0.9086 - val_loss: 0.4880 - val_accuracy: 0.7995\n",
      "Epoch 28/30\n",
      "250/250 [==============================] - 68s 270ms/step - loss: 0.2021 - accuracy: 0.9156 - val_loss: 0.5447 - val_accuracy: 0.8030\n",
      "Epoch 29/30\n",
      "250/250 [==============================] - 75s 302ms/step - loss: 0.2088 - accuracy: 0.9162 - val_loss: 0.5352 - val_accuracy: 0.8085\n",
      "Epoch 30/30\n",
      "250/250 [==============================] - 81s 323ms/step - loss: 0.1971 - accuracy: 0.9206 - val_loss: 0.5892 - val_accuracy: 0.7950\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1fb5fdb65b0>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn.fit(x = train_generator, validation_data = validation_generator, epochs = 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be5fa9e1-a3ba-4e37-8908-96093bf403e2",
   "metadata": {},
   "source": [
    "### 4 Making a single Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5c600f95-83a3-41b7-9928-925185c2cf49",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dog\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.preprocessing import image\n",
    "test_image = image.load_img('dataset/single_prediction/cat_or_dog_1.jpg', target_size = (64,64))\n",
    "test_image = image.img_to_array(test_image)\n",
    "test_image = np.expand_dims(test_image, axis = 0)\n",
    "result = cnn.predict(test_image)\n",
    "train_generator.class_indices\n",
    "if result[0][0] == 1 :\n",
    "    prediction = 'dog'\n",
    "else:\n",
    "    predition = 'cat'\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed0b4ff3-ae5c-43ce-adf1-5caee77cc73d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
